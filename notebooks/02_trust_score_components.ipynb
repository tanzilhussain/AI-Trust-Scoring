{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cacbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch \n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "import textstat\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23f2bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# goal: sentiment- detect emotional warmth, empathy, tone\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d454aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_score ={\n",
    "    'LABEL_0': 0.0,  # negative\n",
    "    'LABEL_1': 0.5,  # neutral\n",
    "    'LABEL_2': 1.0,  # positive\n",
    "}\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    try:\n",
    "        result = sentiment_pipe(text[:512][0])\n",
    "        print(\"senty: \", result)\n",
    "        dict = result[0]\n",
    "        print(round(dict['score'], 2))\n",
    "        # get the assigned label or default to .5\n",
    "        return round(dict['score'], 2)\n",
    "    except Exception as e:\n",
    "        print(\"error with sentiment score: \", e)\n",
    "        return 0.5\n",
    "\n",
    "# goal: readability- check if text is clearly written\n",
    "def get_readability_score(text):\n",
    "    result = textstat.flesch_reading_ease(text) # from 0-100, good is 70-80\n",
    "    result = max(0, min(result/100,1)) # normalize\n",
    "    return result\n",
    "\n",
    "# goal: specificity- how detailed or vague is the text?\n",
    "def get_specificity_score(text):\n",
    "    doc = nlp(text)\n",
    "    detail_words = [token for token in doc if {\"NOUN\", \"PROPN\", \"VERB\"}] # only flags nouns and words bc those contribute to specificity the most\n",
    "    if len(doc) > 0:\n",
    "        return len(detail_words) / len(doc)\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "# goal: personalization- how personal is the advice\n",
    "def get_personalization_score(text):\n",
    "    personal_words = [\"you\", \"your\", \"yours\", \"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\"]\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        count = sum(1 for word in doc if word in personal_words)\n",
    "    return count\n",
    "\n",
    "\n",
    "def total_trust_score(text):\n",
    "    total = (.25 * get_personalization_score(text)) + (.25 * get_specificity_score(text)) +(.25 * get_readability_score(text)) + (.25 * get_sentiment_score(text))\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af79837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.48526865756541526\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.5198447963800905\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.5402384815436242\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.4850145970394737\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.5466581683168317\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.48839407894736847\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5068966746330261}]\n",
      "0.51\n",
      "0.5356695308110133\n",
      "0.0\n",
      "0\n",
      "senty:  [{'label': 'LABEL_1', 'score': 0.5039997696876526}]\n",
      "0.5\n",
      "0.5247079941860465\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"data/dear_abby_scored.jsonl\")\n",
    "data = []\n",
    "with output_path.open(\"r\") as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()}\\nError: {e}\")\n",
    "\n",
    "for line in data:\n",
    "    total = 0.0\n",
    "    text = line[\"ai_advice\"]\n",
    "    print(get_personalization_score(text))\n",
    "    print(total_trust_score(text))\n",
    "    print(total)\n",
    "#     line[\"trust_score\"] = total\n",
    "\n",
    "# with output_path.open(\"w\") as f:\n",
    "#     for line in data:\n",
    "#         f.write(json.dumps(line) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Trust Score Env",
   "language": "python",
   "name": "ai_trust_score_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
